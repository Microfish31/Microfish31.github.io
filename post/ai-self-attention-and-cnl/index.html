<!DOCTYPE html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  
    class="html theme--light"
  
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
    Microfish
        |
        Paper Study (On the Relationship between Self-Attention and Convolutional Layers)
      

    

  </title>

  
  <meta charset="utf-8" /><meta name="generator" content="Hugo 0.123.7"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="Microfish" />
  <meta
    name="description"
    content="Love travel , love writing."
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.26d8396ded3f4aa47cc8d77a9fcb09fee1d20d223c6b3f074a21c7e217bc8c6d.css"
      integrity="sha256-Jtg5be0/SqR8yNd6n8sJ/uHSDSI8az8HSiHH4he8jG0="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css"
    integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css"
    integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css"
    integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css"
    integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />

  <link rel="canonical" href="http://localhost:1313/post/ai-self-attention-and-cnl/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js"
      integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc="
      crossorigin="anonymous"
    ></script>
  

  


  
  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Paper Study (On the Relationship between Self-Attention and Convolutional Layers)"/>
<meta name="twitter:description" content="Study for paper &ldquo;On the Relationship between Self-Attention and Convolutional Layers&rdquo;."/>



  
  <meta property="og:title" content="Paper Study (On the Relationship between Self-Attention and Convolutional Layers)" />
<meta property="og:description" content="Study for paper &ldquo;On the Relationship between Self-Attention and Convolutional Layers&rdquo;." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/post/ai-self-attention-and-cnl/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-08-04T22:19:19+08:00" />
<meta property="article:modified_time" content="2024-08-04T22:19:19+08:00" /><meta property="og:site_name" content="Microfish" />




  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "Paper Study (On the Relationship between Self-Attention and Convolutional Layers)",
        "headline": "Paper Study (On the Relationship between Self-Attention and Convolutional Layers)",
        "alternativeHeadline": "",
        "description": "
      
        \u003cp\u003eStudy for paper \u0026ldquo;On the Relationship between Self-Attention and Convolutional Layers\u0026rdquo;.\u003c\/p\u003e


      


    ",
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/post\/ai-self-attention-and-cnl\/"
        },
        "author" : {
            "@type": "Person",
            "name": "Microfish"
        },
        "creator" : {
            "@type": "Person",
            "name": "Microfish"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "Microfish"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "Microfish"
        },
        "copyrightYear" : "2024",
        "dateCreated": "2024-08-04T22:19:19.00Z",
        "datePublished": "2024-08-04T22:19:19.00Z",
        "dateModified": "2024-08-04T22:19:19.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Microfish",
            "url": "http://localhost:1313/",
            "logo": {
                "@type": "ImageObject",
                "url": "http:\/\/localhost:1313\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
      ]

    ,
        "url" : "http:\/\/localhost:1313\/post\/ai-self-attention-and-cnl\/",
        "wordCount" : "98",
        "genre" : [ ],
        "keywords" : [ 
      
      "AI"

    ]
    }
  </script>


</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="https://avatars.githubusercontent.com/u/75024370?v=4"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/">Microfish</a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p>Love travel , love writing.</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Microfish
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/"
              
              title=""
              >Home</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/post/"
              
              title=""
              >Posts</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/about/"
              
              title=""
              >About</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/portfolio/"
              
              title=""
              >Portfolio</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/contact/"
              
              title=""
              >Contact</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      <h1>Paper Study (On the Relationship Between Self-Attention and Convolutional Layers)</h1>
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                
                  Sun, Aug 4, 2024
                

              
            </span>
          </li>
          <li class="post__meta-item">
            <em class="fas fa-stopwatch post__meta-icon"></em>
            <span class="post__meta-text">One-minute read</span>
          </li>
        </ul>
      <p>Study for paper &ldquo;On the Relationship between Self-Attention and Convolutional Layers&rdquo;.</p>
<h2 id="paper">Paper</h2>
<p><strong>Title:</strong> On the Relationship between Self-Attention and Convolutional Layers<br>
<strong>Authors:</strong> Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi<br>
<strong>Publisher:</strong> ICLR<br>
<strong>Published in:</strong> ???<br>
<strong>Date of Publication:</strong> 2020
<strong>Source Literature:</strong> <a href="https://arxiv.org/abs/1911.03584">https://arxiv.org/abs/1911.03584</a></p>
<h2 id="summary">Summary</h2>
<h2 id="references">References</h2>
<ul>
<li><a href="https://sh-tsang.medium.com/brief-review-on-the-relationship-between-self-attention-and-convolutional-layers-509a75230478">Brief Review — On the Relationship between Self-Attention and Convolutional Layers</a></li>
<li><a href="https://maadotaa.medium.com/self-attention-in-convolutional-neural-networks-172d947afc00">Self Attention in Convolutional Neural Networks</a></li>
</ul>
<p>Local self-attention</p>
<h3 id="others">Others</h3>
<p>二次編碼（quadratic encoding）嘗試使用一個二次函數來描述輸出與輸入之間的關係。在數學中，二次函數是一種多項式函數，其形式通常為：</p>
<p>$$
[ f(x) = ax^2 + bx + c ]
$$</p>
<p>其中，(x) 是輸入變量，而 (f(x)) 是輸出結果。二次編碼的目標是找出一個類似這樣的函數來表示輸出值，這個輸出是根據輸入值計算得出的。</p>
<p>具體來說，這種方法試圖確保輸出不僅與輸入值直接相關，還能捕捉到輸入變量之間的相互關係和非線性關係。通過這樣的編碼，模型可以更有效地表示和預測複雜的輸入-輸出關係，例如人形機器人的手部姿勢或其他動態系統。</p>
<h3 id="qa">Q/A</h3>
<ol>
<li>
<p>在用 self attension 模擬 cnn ，每個 query pixel 對附近的pixel 做 attention 就是卷積嗎?
是的，您說得沒錯！在使用自注意力機制（self-attention）模擬卷積神經網絡（CNN）的過程中，每個查詢像素對其鄰近像素進行注意力計算，這確實可以看作是一種卷積操作的形式。
使用自注意力機制時，每個查詢像素都會與其鄰近的像素（通常是與其在一定範圍內的像素）進行互動。這種互動通常是通過計算查詢像素和鄰近像素之間的相似性（即注意力權重）來實現的。然後，每個查詢像素的最終表示是鄰近像素表示的加權和，其中的權重是通過注意力機制學習得到的。</p>
</li>
<li>
<p>多個頭使得模型可以在不同的子空間中進行注意力計算，從而在整體上增加了模型的靈活性。也就是說同樣範圍內做同件事情 但是有不同的結果?
在多頭注意力中，所有頭的結果會被聯合（concatenate）在一起，然後通過一個最終的線性變換來整合。這樣做的目的是將各個頭學到的不同特徵融合起來，形成一個更加豐富和全面的表示。</p>
</li>
</ol></div>
    <div class="post__footer">
      

      
        <span><a class="tag" href="/tags/ai/">AI</a></span>


      
    </div>

    <div id="comment">
          <h2>comments</h2>
          <div id="commento"></div>
<script defer src="https://cdn.commento.io/js/commento.js"></script>
<noscript>Please enable JavaScript to load the comments.</noscript>

        </div>
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Microfish
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></body>
</html>

<script src="https://unpkg.com/mermaid@8.13.4/dist/mermaid.min.js"></script>
<script>
  mermaid.initialize({ startOnLoad: true });
</script>